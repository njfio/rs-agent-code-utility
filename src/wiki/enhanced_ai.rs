//! Enhanced AI Integration Module for Wiki Generation
//!
//! This module provides advanced AI context and documentation generation
//! by leveraging existing AI services and security analysis capabilities.
//!
//! It extends the basic wiki functionality with:
//! - Rich context building for AI prompts
//! - Security vulnerability explanations
//! - Code quality assessments
//! - Function-level documentation enhancement
//! - Refactoring suggestions integration
//! - Diagram annotations with AI insights

use crate::analyzer::FileInfo;
use crate::advanced_security::SecurityVulnerability;
use crate::ai::service::AIService;
use crate::ai::types::{AIRequest, AIFeature, AIProvider};
use crate::error::Result;
use std::collections::HashMap;
use tokio::runtime::Runtime;

/// Enhanced AI context for wiki documentation generation
#[derive(Debug, Clone)]
pub struct AIEnhancementContext {
    /// Function-level context mapping
    pub function_contexts: HashMap<String, FunctionAIContext>,
    /// Security insights for vulnerabilities
    pub security_insights: HashMap<String, SecurityAIInsights>,
    /// Overall module assessment
    pub module_assessment: ModuleAssessment,
}

/// AI-enhanced context for individual functions
#[derive(Debug, Clone)]
pub struct FunctionAIContext {
    /// Original symbol info
    pub symbol_name: String,
    /// Rich documentation generated by AI
    pub enriched_docs: Option<String>,
    /// Refactoring suggestions
    pub refactoring_hints: Vec<String>,
    /// Performance considerations
    pub performance_notes: Option<String>,
    /// Security annotations
    pub security_annotations: Vec<String>,
}

/// AI-based security insights for vulnerabilities
#[derive(Debug, Clone)]
pub struct SecurityAIInsights {
    /// Vulnerability explanation
    pub explanation: Option<String>,
    /// Remediation suggestions
    pub remediation_steps: Vec<String>,
    /// Risk assessment (0.0 to 1.0)
    pub risk_score: Option<f64>,
}

/// Overall module quality assessment
#[derive(Debug, Clone, Default)]
pub struct ModuleAssessment {
    /// Code maintainability score (0-100)
    pub maintainability_score: Option<u32>,
    /// Security posture rating
    pub security_rating: Option<String>,
    /// Key recommendations
    pub recommendations: Vec<String>,
}

/// Configuration flags for enhanced AI features
#[derive(Debug, Clone)]
pub struct EnhancedAIConfig {
    /// Enable rich function documentation
    pub enable_function_enhancement: bool,
    /// Enable security vulnerability explanations
    pub enable_security_insights: bool,
    /// Enable refactoring suggestions
    pub enable_refactoring_hints: bool,
    /// Enable diagram annotations
    pub enable_diagram_annotations: bool,
    /// AI provider to use
    pub ai_provider: AIProvider,
    /// Maximum AI tokens per request
    pub max_tokens_per_request: usize,
    /// Use mock AI for testing
    pub use_mock_ai: bool,
}

impl Default for EnhancedAIConfig {
    fn default() -> Self {
        Self {
            enable_function_enhancement: true,
            enable_security_insights: true,
            enable_refactoring_hints: true,
            enable_diagram_annotations: false, // Disabled by default for performance
            ai_provider: AIProvider::OpenAI,
            max_tokens_per_request: 1000,
            use_mock_ai: false,
        }
    }
}

/// Enhanced AI service for wiki generation
pub struct WikiAIEnhancer {
    ai_service: AIService,
    config: EnhancedAIConfig,
    runtime: Runtime,
}

impl WikiAIEnhancer {
    /// Create a new AI enhancer
    pub async fn new(config: EnhancedAIConfig) -> Result<Self> {
        let mut builder = crate::ai::service::AIServiceBuilder::new()
            .with_default_provider(config.ai_provider.clone());

        if config.use_mock_ai {
            builder = builder.with_mock_providers(true);
        }

        let ai_service = builder.build().await?;

        // Create tokio runtime for blocking calls
        let runtime = Runtime::new().map_err(|e| crate::error::Error::internal_error(
            "enhanced_ai",
            format!("Failed to create tokio runtime: {}", e)
        ))?;

        Ok(Self {
            ai_service,
            config,
            runtime,
        })
    }

    /// Generate enhanced AI context for a file
    pub fn generate_file_context(&self, file_info: &FileInfo) -> Result<AIEnhancementContext> {
        let mut context = AIEnhancementContext {
            function_contexts: HashMap::new(),
            security_insights: HashMap::new(),
            module_assessment: ModuleAssessment::default(),
        };

        // Generate function-level enhancements
        for symbol in &file_info.symbols {
            if symbol.kind.contains("function") || symbol.kind.contains("fn") {
                let function_context = self.generate_function_context(symbol, file_info)?;
                context.function_contexts.insert(symbol.name.clone(), function_context);
            }
        }

        // Generate security insights for vulnerabilities
        for vulnerability in &file_info.security_vulnerabilities {
            let insights = self.generate_security_insights(vulnerability)?;
            context.security_insights.insert(vulnerability.unique_id.clone(), insights);
        }

        // Generate overall module assessment
        context.module_assessment = self.assess_module_quality(file_info)?;

        Ok(context)
    }

    /// Generate enriched context for a function
    fn generate_function_context(&self, symbol: &crate::analyzer::Symbol, file_info: &FileInfo) -> Result<FunctionAIContext> {
        let mut context = FunctionAIContext {
            symbol_name: symbol.name.clone(),
            enriched_docs: None,
            refactoring_hints: Vec::new(),
            performance_notes: None,
            security_annotations: Vec::new(),
        };

        // Generate enriched documentation if enabled
        if self.config.enable_function_enhancement {
            context.enriched_docs = self.generate_function_documentation(symbol, file_info).ok();
        }

        // Generate refactoring hints if enabled
        if self.config.enable_refactoring_hints {
            context.refactoring_hints = self.generate_refactoring_suggestions(symbol, file_info)?;
        }

        // Add security annotations based on vulnerabilities
        if self.config.enable_security_insights {
            context.security_annotations = self.extract_security_annotations(symbol, file_info);
        }

        Ok(context)
    }

    /// Generate enriched documentation for a function
    fn generate_function_documentation(&self, symbol: &crate::analyzer::Symbol, file_info: &FileInfo) -> Result<String> {
        let content = format!(
            "Generate comprehensive documentation for the function '{}':\n\
            Context: This function is in file {} ({} lines, {} symbols)\n\
            Original documentation: {}\n\
            Function signature: {} (kind: {})\n\
            Lines: {}-{}\n\n\
            Please provide enriched documentation that includes:\n\
            1. Purpose and functionality\n\
            2. Parameters and return values (inferred)\n\
            3. Usage examples\n\
            4. Performance considerations\n\
            5. Error conditions",
            symbol.name,
            file_info.path.display(),
            file_info.lines,
            file_info.symbols.len(),
            symbol.documentation.as_deref().unwrap_or("No documentation"),
            symbol.name,
            symbol.kind,
            symbol.start_line,
            symbol.end_line,
        );

        let request = AIRequest::new(AIFeature::DocumentationGeneration, content)
            .with_temperature(0.3)
            .with_max_tokens(self.config.max_tokens_per_request);

        let response = self.runtime.block_on(async {
            self.ai_service.process_request(request).await
        })?;

        Ok(response.content)
    }

    /// Generate refactoring suggestions for a function
    fn generate_refactoring_suggestions(&self, symbol: &crate::analyzer::Symbol, file_info: &FileInfo) -> Result<Vec<String>> {
        let content = format!(
            "Analyze the function '{}' for potential refactoring improvements:\n\
            Current implementation: {} lines (line {}-{})\n\
            Function kind: {}\n\
            File: {}\n\n\
            Provide specific, actionable refactoring suggestions focusing on:\n\
            1. Code complexity reduction\n\
            2. Improved readability\n\
            3. Performance optimizations\n\
            4. Error handling improvements\n\
            5. Design pattern adherence",
            symbol.name,
            symbol.end_line - symbol.start_line + 1,
            symbol.start_line,
            symbol.end_line,
            symbol.kind,
            file_info.path.display()
        );

        let request = AIRequest::new(AIFeature::RefactoringSuggestions, content)
            .with_temperature(0.4)
            .with_max_tokens(self.config.max_tokens_per_request);

        let response = self.runtime.block_on(async {
            self.ai_service.process_request(request).await
        })?;

        // Parse response into vector of suggestions
        let suggestions = response.content
            .lines()
            .filter(|line| !line.trim().is_empty())
            .map(|line| line.trim_start_matches(|c: char| !c.is_alphabetic()).to_string())
            .filter(|line| line.len() > 10)
            .collect();

        Ok(suggestions)
    }

    /// Generate security insights for a vulnerability
    fn generate_security_insights(&self, vulnerability: &SecurityVulnerability) -> Result<SecurityAIInsights> {
        let content = format!(
            "Provide security insights for this vulnerability:\n\
            Vulnerability: {}\n\
            Severity: {}\n\
            Description: {}\n\
            Location: {}:{}\n\
            Category: {}\n\n\
            Please provide:\n\
            1. Explanation of the vulnerability in clear terms\n\
            2. Specific remediation steps\n\
            3. Risk assessment score (0.0 to 1.0)\n\
            4. Preventive measures for similar vulnerabilities",
            vulnerability.title,
            vulnerability.severity,
            vulnerability.description,
            vulnerability.location.file.display(),
            vulnerability.location.line,
            vulnerability.category
        );

        let request = AIRequest::new(AIFeature::SecurityAnalysis, content)
            .with_temperature(0.2)
            .with_max_tokens(self.config.max_tokens_per_request);

        let response = self.runtime.block_on(async {
            self.ai_service.process_request(request).await
        })?;

        // Parse the response
        let insight_lines: Vec<&str> = response.content.lines().collect();

        let explanation = insight_lines
            .iter()
            .find(|line| line.to_lowercase().contains("explanation"))
            .map(|s| s.to_string());

        let remediation_steps: Vec<String> = insight_lines
            .iter()
            .filter(|line| line.to_lowercase().contains("remediation") ||
                          line.to_lowercase().contains("fix") ||
                          line.to_lowercase().contains("recommend"))
            .map(|line| line.to_string())
            .collect();

        let risk_score = insight_lines
            .iter()
            .find(|line| line.to_lowercase().contains("risk") ||
                        line.contains("score"))
            .and_then(|line| {
                // Extract numeric value from line
                line.chars()
                    .filter(|c| c.is_numeric() || *c == '.')
                    .collect::<String>()
                    .parse::<f64>()
                    .ok()
            });

        Ok(SecurityAIInsights {
            explanation,
            remediation_steps,
            risk_score,
        })
    }

    /// Assess overall module quality
    fn assess_module_quality(&self, file_info: &FileInfo) -> Result<ModuleAssessment> {
        let content = format!(
            "Assess the overall quality of this code module:\n\
            File: {}\n\
            Size: {} lines\n\
            Total symbols: {}\n\
            Functions: {}\n\
            Security vulnerabilities: {}\n\
            Parse errors: {}\n\n\
            Provide:\\n1. Maintainability score (0-100)\\n2. Security rating\\n3. Key recommendations",
            file_info.path.display(),
            file_info.lines,
            file_info.symbols.len(),
            file_info.symbols.iter().filter(|s| s.kind.contains("function")).count(),
            file_info.security_vulnerabilities.len(),
            file_info.parse_errors.len()
        );

        let request = AIRequest::new(AIFeature::CodeQualityAssessment, content)
            .with_temperature(0.3)
            .with_max_tokens(self.config.max_tokens_per_request);

        let response = self.runtime.block_on(async {
            self.ai_service.process_request(request).await
        })?;

        let lines: Vec<&str> = response.content.lines().collect();

        let maintainability_score = lines
            .iter()
            .find(|line| line.to_lowercase().contains("maintainability"))
            .and_then(|line| {
                line.chars()
                    .filter(|c| c.is_numeric())
                    .collect::<String>()
                    .parse::<u32>()
                    .ok()
            });

        let security_rating = lines
            .iter()
            .find(|line| line.to_lowercase().contains("security") &&
                        line.to_lowercase().contains("rating"))
            .map(|s| s.to_string());

        let recommendations: Vec<String> = lines
            .iter()
            .filter(|line| line.to_lowercase().contains("recommend") ||
                          line.to_lowercase().contains("suggest") ||
                          line.to_lowercase().contains("consider"))
            .map(|line| line.to_string())
            .collect();

        Ok(ModuleAssessment {
            maintainability_score,
            security_rating,
            recommendations,
        })
    }

    /// Extract security annotations for a specific symbol
    fn extract_security_annotations(&self, symbol: &crate::analyzer::Symbol, file_info: &FileInfo) -> Vec<String> {
        let mut annotations = Vec::new();

        for vuln in &file_info.security_vulnerabilities {
            // Check if vulnerability is related to this symbol (rough heuristic)
            if vuln.location.line >= symbol.start_line &&
               vuln.location.line <= symbol.end_line {
                annotations.push(format!("⚠️ Security: {}", vuln.title));
            }
        }

        annotations
    }

    /// Generate diagram annotations with AI insights
    pub fn generate_diagram_annotations(&self, file_info: &FileInfo) -> Result<HashMap<String, String>> {
        if !self.config.enable_diagram_annotations {
            return Ok(HashMap::new());
        }

        let mut annotations = HashMap::new();

        for symbol in &file_info.symbols {
            if symbol.kind.contains("function") {
                let content = format!(
                    "Generate a short diagram annotation/label for function '{}':",
                    symbol.name
                );

                let request = AIRequest::new(AIFeature::DocumentationGeneration, content)
                    .with_temperature(0.5)
                    .with_max_tokens(50);

                match self.runtime.block_on(async {
                    self.ai_service.process_request(request).await
                }) {
                    Ok(response) => {
                        annotations.insert(symbol.name.clone(), response.content);
                    }
                    Err(_) => {
                        // Use fallback annotation
                        annotations.insert(symbol.name.clone(),
                            format!("Function: {}", symbol.name));
                    }
                }
            }
        }

        Ok(annotations)
    }
}

/// Helper function to safely generate AI content with fallbacks
pub fn generate_ai_content_with_fallback<F>(
    enhancer: &WikiAIEnhancer,
    generate_fn: F,
    fallback: String
) -> String
where
    F: FnOnce(&WikiAIEnhancer) -> Result<String>,
{
    match generate_fn(enhancer) {
        Ok(content) if !content.trim().is_empty() => content,
        _ => fallback,
    }
}
